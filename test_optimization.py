"""Test EdgeFlow Optimization Pipeline

This script tests the core optimization functionality without relying on
the parser, to demonstrate that EdgeFlow's ML optimization works.
"""

import json
import logging
from optimizer import optimize
from benchmarker import benchmark_model, compare_models

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_edgeflow_optimization():
    """Test the complete EdgeFlow optimization pipeline."""
    print("=" * 60)
    print("ğŸš€ EdgeFlow Optimization Pipeline Test")
    print("   Testing core ML optimization functionality")
    print("=" * 60)
    
    # Test configuration (simulating what would come from parser)
    test_configs = [
        {
            "model": "test_model.tflite",
            "quantize": "int8",
            "target_device": "raspberry_pi",
            "deploy_path": "generated",
            "optimize_for": "latency",
            "memory_limit": 64,
            "enable_fusion": True
        },
        {
            "model": "test_model.tflite", 
            "quantize": "float16",
            "target_device": "jetson_nano",
            "deploy_path": "generated",
            "optimize_for": "memory",
            "memory_limit": 128,
            "enable_fusion": True
        },
        {
            "model": "test_model.tflite",
            "quantize": "none",
            "target_device": "cpu",
            "deploy_path": "generated", 
            "optimize_for": "accuracy",
            "memory_limit": 256,
            "enable_fusion": False
        }
    ]
    
    all_results = []
    
    for i, config in enumerate(test_configs, 1):
        print(f"\nğŸ“ Test {i}: {config['quantize'].upper()} quantization for {config['target_device']}")
        print("-" * 50)
        
        try:
            # Run optimization
            logger.info(f"Running optimization for config {i}")
            optimized_path, opt_results = optimize(config)
            
            # Benchmark original model
            logger.info("Benchmarking original model")
            original_benchmark = benchmark_model(config['model'], config)
            
            # Benchmark optimized model
            logger.info("Benchmarking optimized model")
            optimized_benchmark = benchmark_model(optimized_path, config)
            
            # Compare models
            logger.info("Comparing models")
            comparison = compare_models(config['model'], optimized_path, config)
            
            # Show results
            improvements = comparison.get('improvements', {})
            print(f"âœ… Optimization successful!")
            print(f"   Model size reduction: {improvements.get('size_reduction_percent', 0):.1f}%")
            print(f"   Latency improvement: {improvements.get('latency_improvement_percent', 0):.1f}%")
            print(f"   Throughput improvement: {improvements.get('throughput_improvement_percent', 0):.1f}%")
            print(f"   Memory improvement: {improvements.get('memory_improvement_percent', 0):.1f}%")
            print(f"   Optimized model: {optimized_path}")
            
            # Store results
            test_result = {
                'config': config,
                'optimization': opt_results,
                'original_benchmark': original_benchmark,
                'optimized_benchmark': optimized_benchmark,
                'comparison': comparison,
                'status': 'success'
            }
            all_results.append(test_result)
            
        except Exception as e:
            logger.error(f"Test {i} failed: {e}")
            test_result = {
                'config': config,
                'error': str(e),
                'status': 'failed'
            }
            all_results.append(test_result)
    
    # Summary
    print(f"\nğŸ“Š Test Summary")
    print("=" * 60)
    successful_tests = [r for r in all_results if r['status'] == 'success']
    failed_tests = [r for r in all_results if r['status'] == 'failed']
    
    print(f"âœ… Successful tests: {len(successful_tests)}/{len(all_results)}")
    print(f"âŒ Failed tests: {len(failed_tests)}/{len(all_results)}")
    
    if successful_tests:
        print(f"\nğŸ¯ Best Results:")
        best_size_reduction = max(successful_tests, 
                                key=lambda x: x['comparison']['improvements'].get('size_reduction_percent', 0))
        best_latency = max(successful_tests,
                          key=lambda x: x['comparison']['improvements'].get('latency_improvement_percent', 0))
        
        print(f"   Best size reduction: {best_size_reduction['config']['quantize']} on {best_size_reduction['config']['target_device']} "
              f"({best_size_reduction['comparison']['improvements'].get('size_reduction_percent', 0):.1f}%)")
        print(f"   Best latency improvement: {best_latency['config']['quantize']} on {best_latency['config']['target_device']} "
              f"({best_latency['comparison']['improvements'].get('latency_improvement_percent', 0):.1f}%)")
    
    # Save detailed results
    with open('edgeflow_test_results.json', 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nğŸ’¾ Detailed results saved to: edgeflow_test_results.json")
    
    # Final verdict
    if len(successful_tests) > 0:
        print(f"\nğŸ‰ SUCCESS! EdgeFlow optimization pipeline works!")
        print("=" * 60)
        print("âœ… Model quantization (INT8, Float16) works")
        print("âœ… Device-specific optimization works")
        print("âœ… Benchmarking and comparison works")
        print("âœ… EdgeFlow delivers measurable improvements")
        print("âœ… Ready for hackathon presentation!")
        return True
    else:
        print(f"\nâŒ FAILED! EdgeFlow optimization pipeline has issues")
        return False

if __name__ == "__main__":
    success = test_edgeflow_optimization()
    exit(0 if success else 1)
